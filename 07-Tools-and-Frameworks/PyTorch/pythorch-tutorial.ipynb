{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2dcd46",
   "metadata": {},
   "source": [
    "API for common tasks in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5a3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f838f1f",
   "metadata": {},
   "source": [
    "The code below loads the FashionMNIST dataset using Pytorch's torchvision.datasets module.\n",
    "\n",
    "**About FashionMNIST**\n",
    "It's a dataset of 70,000 grayscale images (28×28 pixels) across 10 clothing categories:\n",
    "\n",
    "T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbacb129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0ed8c",
   "metadata": {},
   "source": [
    "## DataLoader Setup\n",
    "This code below wraps the datasets into DataLoaders, which handle batching and iteration during training/testing.\n",
    "\n",
    "batch_size = 64\n",
    "Instead of feeding all 60,000 images at once, the data is split into batches of 64 images. This is more memory-efficient and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c89011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452993bc",
   "metadata": {},
   "source": [
    "## Neural Network Definition\n",
    "\n",
    "Below we create a model that will be trained to predict the clothes' categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937375a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dc8db",
   "metadata": {},
   "source": [
    "## Loss Function & Optimizer\n",
    "These two lines below define how the model learns.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "The loss function measures how wrong the model's predictions are.\n",
    "\n",
    "Takes the model's 10 output logits and the correct label\n",
    "Returns a single number — the error score\n",
    "Lower = better\n",
    "Example: if the model predicts \"Bag\" with 90% confidence but the answer is \"Sneaker\", the loss will be high. If it predicts \"Sneaker\" correctly with high confidence, the loss will be low.\n",
    "\n",
    "CrossEntropyLoss is commonly used for multi-class classification problems like this one (10 categories).\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "The optimizer is responsible for updating the model's weights to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238ea53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f81424",
   "metadata": {},
   "source": [
    "## The train Function\n",
    "This function performs one full pass through the training data, updating the model's weights on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb3ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801ca10",
   "metadata": {},
   "source": [
    "## The test Function\n",
    "This function evaluates the model's performance on unseen data — no learning happens here, only measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553830e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d0edd",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "This is the main training loop that ties everything together by running train and test repeatedly to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4988c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302256  [   64/60000]\n",
      "loss: 2.295820  [ 6464/60000]\n",
      "loss: 2.273208  [12864/60000]\n",
      "loss: 2.264461  [19264/60000]\n",
      "loss: 2.242330  [25664/60000]\n",
      "loss: 2.217479  [32064/60000]\n",
      "loss: 2.219109  [38464/60000]\n",
      "loss: 2.186470  [44864/60000]\n",
      "loss: 2.181127  [51264/60000]\n",
      "loss: 2.145282  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 2.145149 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.153536  [   64/60000]\n",
      "loss: 2.147100  [ 6464/60000]\n",
      "loss: 2.085178  [12864/60000]\n",
      "loss: 2.096226  [19264/60000]\n",
      "loss: 2.038590  [25664/60000]\n",
      "loss: 1.986362  [32064/60000]\n",
      "loss: 2.003953  [38464/60000]\n",
      "loss: 1.926739  [44864/60000]\n",
      "loss: 1.929425  [51264/60000]\n",
      "loss: 1.850182  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.855980 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.889800  [   64/60000]\n",
      "loss: 1.863339  [ 6464/60000]\n",
      "loss: 1.742610  [12864/60000]\n",
      "loss: 1.776304  [19264/60000]\n",
      "loss: 1.666419  [25664/60000]\n",
      "loss: 1.634988  [32064/60000]\n",
      "loss: 1.642598  [38464/60000]\n",
      "loss: 1.554971  [44864/60000]\n",
      "loss: 1.581577  [51264/60000]\n",
      "loss: 1.475803  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 1.498242 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.564963  [   64/60000]\n",
      "loss: 1.535559  [ 6464/60000]\n",
      "loss: 1.385218  [12864/60000]\n",
      "loss: 1.451644  [19264/60000]\n",
      "loss: 1.341585  [25664/60000]\n",
      "loss: 1.348964  [32064/60000]\n",
      "loss: 1.350789  [38464/60000]\n",
      "loss: 1.284212  [44864/60000]\n",
      "loss: 1.321105  [51264/60000]\n",
      "loss: 1.225320  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.250494 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.324625  [   64/60000]\n",
      "loss: 1.311135  [ 6464/60000]\n",
      "loss: 1.143407  [12864/60000]\n",
      "loss: 1.245306  [19264/60000]\n",
      "loss: 1.131718  [25664/60000]\n",
      "loss: 1.160943  [32064/60000]\n",
      "loss: 1.169034  [38464/60000]\n",
      "loss: 1.114037  [44864/60000]\n",
      "loss: 1.154778  [51264/60000]\n",
      "loss: 1.073774  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.093647 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd08135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e66359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23d11f",
   "metadata": {},
   "source": [
    "## Making a Prediction\n",
    "This is the inference step — using the trained model to classify a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b971a8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
